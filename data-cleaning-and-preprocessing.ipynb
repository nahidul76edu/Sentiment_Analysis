{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd # data processing\nimport numpy as np\n\n#IMDB Movie Reviews dataset.\nimdb_dataset = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv',  encoding='utf-8')\n\n########################################################################################################################################\n\n#Twitter US Airline dataset.\nUSAirline_dataset = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv',  encoding='utf-8')\n# combaine negative reason with  tweet\nUSAirline_dataset['review'] = USAirline_dataset['negativereason'].fillna('') + ' ' + USAirline_dataset['text'] \nUSAirline_dataset['sentiment'] = USAirline_dataset['airline_sentiment']\n# Selecting specific columns from the original dataset\nUSAirline_dataset = USAirline_dataset[['review', 'sentiment']]\n\n########################################################################################################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom termcolor import cprint\nimport seaborn as sns\nimport warnings   \n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Display the count of different sentiments\ncprint(\"Total number of sentiments of tweets:\", 'green')\nprint(imdb_dataset.sentiment.value_counts())\n\n# Set up the figure for subplots\nplt.figure(figsize=(14, 5))  # Adjust overall figure size\n\n# First subplot: Countplot of sentiments\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nax1 = sns.countplot(x='sentiment', data=imdb_dataset)\n# Iterate through the list of axes' patches (bars)\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.0f'),  # Get the height of each bar (count)\n                 (p.get_x() + p.get_width() / 2., p.get_height()),  # Position for the label\n                 ha = 'center', va = 'center',  # Center alignment\n                 xytext = (0, 9),  # Distance from the top of the bar\n                 textcoords = 'offset points')\n\n# Second subplot: Pie chart of sentiments distribution\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\ncolors = sns.color_palette('husl', 10)  # Define a color palette\nsentiment_counts = imdb_dataset['sentiment'].value_counts()\nsentiment_counts.plot(\n    kind='pie',\n    colors=colors,\n    labels=sentiment_counts.index,  # Automatically use labels from the sentiment column\n    explode=[0.05] * len(sentiment_counts),  # Dynamically generate explode values based on the number of categories\n    shadow=True,\n    autopct='%.2f%%',  # Show the percentage of each sentiment\n    fontsize=12,\n    figsize=(12, 5)\n)\n\nplt.tight_layout()  # Adjust subplots to fit into the figure area.\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom termcolor import cprint\n\n# Display the count of different sentiments\ncprint(\"Total number of sentiments of tweets:\", 'green')\nprint(USAirline_dataset.sentiment.value_counts())\n\n# Set up the figure for subplots\nplt.figure(figsize=(14, 5))  # Adjust overall figure size\n\n# First subplot: Countplot of sentiments\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nax1 = sns.countplot(x='sentiment', data=USAirline_dataset)\n# Iterate over each bar in the countplot and annotate the height (count)\nfor bar in ax1.patches:\n    ax1.annotate(f'{int(bar.get_height())}',  # The count as text\n                 (bar.get_x() + bar.get_width() / 2, bar.get_height()),  # Position for the annotation\n                 ha='center', va='center',  # Center the text horizontally and vertically\n                 xytext=(0, 8),  # Position text slightly above the bar\n                 textcoords='offset points')\n\n# Second subplot: Pie chart of sentiments distribution\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\ncolors = sns.color_palette('husl', 10)  # Define a color palette\nsentiment_counts = USAirline_dataset['sentiment'].value_counts()\nsentiment_counts.plot(\n    kind='pie',\n    colors=colors,\n    labels=sentiment_counts.index,  # Automatically use labels from the sentiment column\n    explode=[0.05] * len(sentiment_counts),  # Dynamically generate explode values based on the number of categories\n    shadow=True,\n    autopct='%.2f%%',  # Show the percentage of each sentiment\n    fontsize=12,\n    figsize=(12, 5)\n)\n\nplt.tight_layout()  # Adjust subplots to fit into the figure area.\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Data cleaning protocol","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport emoji\nfrom bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\nimport warnings\n# Ensure this is imported for handling punctuation\nimport re,string,unicodedata\n\nimport spacy\nfrom spacy.lang.en import English\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Your contractions dictionary\ncontractions_dict = {\n    \"I'm\": \"I am\",\n    \"I'm'a\": \"I am about to\",\n    \"I'm'o\": \"I am going to\",\n    \"I've\": \"I have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"Whatcha\": \"What are you\",\n    \"amn't\": \"am not\",\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"'cause\": \"because\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"daren't\": \"dare not\",\n    \"daresn't\": \"dare not\",\n    \"dasn't\": \"dare not\",\n    \"didn't\": \"did not\",\n    \"didn’t\": \"did not\",\n    \"don't\": \"do not\",\n    \"don’t\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"e'er\": \"ever\",\n    \"everyone's\": \"everyone is\",\n    \"finna\": \"fixing to\",\n    \"gimme\": \"give me\",\n    \"gon't\": \"go not\",\n    \"gonna\": \"going to\",\n    \"gotta\": \"got to\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he've\": \"he have\",\n    \"he's\": \"he is\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"here's\": \"here is\",\n    \"how're\": \"how are\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how's\": \"how is\",\n    \"how'll\": \"how will\",\n    \"isn't\": \"is not\",\n    \"it's\": \"it is\",\n    \"'tis\": \"it is\",\n    \"'twas\": \"it was\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"kinda\": \"kind of\",\n    \"let's\": \"let us\",\n    \"luv\": \"love\",\n    \"ma'am\": \"madam\",\n    \"may've\": \"may have\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"ne'er\": \"never\",\n    \"o'\": \"of\",\n    \"o'clock\": \"of the clock\",\n    \"ol'\": \"old\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"o'er\": \"over\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shalln't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she's\": \"she is\",\n    \"she'll\": \"she will\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"somebody's\": \"somebody is\",\n    \"someone's\": \"someone is\",\n    \"something's\": \"something is\",\n    \"sux\": \"sucks\",\n    \"that're\": \"that are\",\n    \"that's\": \"that is\",\n    \"that'll\": \"that will\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"'em\": \"them\",\n    \"there're\": \"there are\",\n    \"there's\": \"there is\",\n    \"there'll\": \"there will\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"these're\": \"these are\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"this's\": \"this is\",\n    \"this'll\": \"this will\",\n    \"this'd\": \"this would\",\n    \"those're\": \"those are\",\n    \"to've\": \"to have\",\n    \"wanna\": \"want to\",\n    \"wasn't\": \"was not\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"weren't\": \"were not\",\n    \"what're\": \"what are\",\n    \"what'd\": \"what did\",\n    \"what've\": \"what have\",\n    \"what's\": \"what is\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"when've\": \"when have\",\n    \"when's\": \"when is\",\n    \"where're\": \"where are\",\n    \"where'd\": \"where did\",\n    \"where've\": \"where have\",\n    \"where's\": \"where is\",\n    \"which's\": \"which is\",\n    \"who're\": \"who are\",\n    \"who've\": \"who have\",\n    \"who's\": \"who is\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who'd\": \"who would\",\n    \"who'd've\": \"who would have\",\n    \"why're\": \"why are\",\n    \"why'd\": \"why did\",\n    \"why've\": \"why have\",\n    \"why's\": \"why is\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"you'll've\": \"you shall have\",\n    \"you'll\": \"you will\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    # chat_words\n    # IMDb-specific chat words\n    \"CGI\": \"Computer-Generated Imagery\",\n    \"VFX\": \"Visual Effects\",\n    \"OST\": \"Original Soundtrack\",\n    \"GOAT\": \"Greatest Of All Time\",\n    # US Airline/Tweets-specific chat words\n    \"ETA\": \"Estimated Time of Arrival\",\n    \"ETD\": \"Estimated Time of Departure\",\n    \"Layover\": \"A break between flights on a longer journey\",\n    \"FFP\": \"Frequent Flyer Program\",\n    \"TSA\": \"Transportation Security Administration\",\n    # Sentiment140-specific chat words\n    \"LOL\": \"Laugh out loud\",\n    \"OMG\": \"Oh my God/goodness\",\n    \"SMH\": \"Shaking my head\",\n    \"LMAO\": \"Laughing my ass off\",\n    \"TTYL\": \"Talk to you later\",\n    \"BRB\": \"Be right back\",\n    \"IMO/IMHO\": \"In my (humble) opinion\",\n    \"NSFW\": \"Not safe for work\",\n    ###\n    \"BRB\": \"Be right back\",\n    \"BTW\": \"By the way\",\n    \"TTYL\": \"Talk to you later\",\n    \"OMW\": \"On my way\",\n    \"SMH/SMDH\": \"Shaking my head/shaking my darn head\",\n    \"LOL\": \"Laugh out loud\",\n    \"TBD\": \"To be determined\", \n    \"IMHO/IMO\": \"In my humble opinion\",\n    \"HMU\": \"Hit me up\",\n    \"IIRC\": \"If I remember correctly\",\n    \"LMK\": \"Let me know\", \n    \"OG\": \"Original gangsters (used for old friends)\",\n    \"FTW\": \"For the win\", \n    \"NVM\": \"Nevermind\",\n    \"OOTD\": \"Outfit of the day\", \n    \"Ngl\": \"Not gonna lie\",\n    \"Rq\": \"real quick\", \n    \"Iykyk\": \"If you know, you know\",\n    \"Ong\": \"On god (I swear)\", \n    \"YAAAS\": \"Yes!\", \n    \"Brt\": \"Be right there\",\n    \"Sm\": \"So much\",\n    \"Ig\": \"I guess\",\n    \"Wya\": \"Where you at\",\n    \"Istg\": \"I swear to god\",\n    \"Hbu\": \"How about you\",\n    \"Atm\": \"At the moment\",\n    \"Asap\": \"As soon as possible\",\n    \"Fyi\": \"For your information\"\n\n }\n\n# 'contractions_dict' is defined\ncontractions_pattern = re.compile('(%s)' % '|'.join([re.escape(k) for k in contractions_dict.keys()]), flags=re.IGNORECASE)\n\n\ndef Data_cleaning_protocol(text):\n    def remove_html(text):\n        # First, remove HTML to avoid HTML tags affecting other text processing steps.\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', MarkupResemblesLocatorWarning)\n            return BeautifulSoup(text, \"html.parser\").get_text()\n\n    def expand_contractions(text):\n        # Expand contractions early to ensure that any punctuation involved in contractions is handled correctly.\n        def replace(match):\n            return contractions_dict.get(match.group(0).lower(), match.group(0))\n        return contractions_pattern.sub(replace, text)\n    \n    def handle_emojis(text):\n        # Handle emojis early to convert them into text, which can be processed by subsequent steps.\n        text = emoji.demojize(text, delimiters=(\" \", \" \"))\n        return text\n\n    def remove_urls_usernames(text):\n        # Remove URLs and usernames to prevent them from being altered by later steps like removing special characters.\n        return re.sub(r'https?://\\S+|www\\.\\S+|@[^\\s]+', '', text)\n\n    def remove_html_linebreaks(text):\n        # Handle HTML line breaks after removing HTML content but before altering the text further.\n        # This targets a specific HTML line break pattern for conversion to space.\n        linebreaks = \"<br /><br />\"\n        return re.sub(linebreaks, \" \", text)\n\n    def to_lowercase(text):\n        # Convert to lowercase early to standardize the case for all subsequent operations.\n        return text.lower()\n    \n    def remove_square_brackets(text):\n        # Remove text within square brackets before general special character removal to target specific unwanted text.\n        return re.sub(r'\\[[^]]*\\]', '', text)\n\n    def remove_special_characters(text, remove_digits=True):\n        # Removing special characters (and optionally digits) after specific patterns like URLs, usernames,\n        # and square brackets have been addressed.\n        pattern = r'[^a-zA-Z\\s]' if remove_digits else r'[^a-zA-Z0-9\\s]'\n        return re.sub(pattern, '', text)\n\n    def reduce_character_sequences(text):\n        # Reduce sequences of repetitive characters after case normalization and special character removal\n        # to clean up any resulting awkwardness from previous steps.\n        sequencePattern = r\"(.)\\1\\1+\"\n        seqReplacePattern = r\"\\1\\1\"\n        return re.sub(sequencePattern, seqReplacePattern, text)\n\n    def remove_hash_symbol(text):\n        # Remove the '#' symbol from hashtags, allowing for clean word extraction, performed after other text cleanups.\n        return re.sub(r'#(\\S+)', r'\\1', text)\n\n    # Applying all preprocessing steps in logical order\n    text = remove_html(text)\n    text = handle_emojis(text)\n    text = expand_contractions(text)\n    text = remove_urls_usernames(text)\n    text = remove_html_linebreaks(text)\n    text = to_lowercase(text)\n    text = remove_square_brackets(text)\n    text = remove_special_characters(text)\n    # Redundant removal of words containing numbers after special characters have been handled\n    text = re.sub('\\w*\\d\\w*', '', text)\n    # Newline characters are removed here to clean up any formatting leftovers.\n    text = re.sub('\\n', '', text)\n    text = reduce_character_sequences(text)\n    text = remove_hash_symbol(text)\n\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Stopwords Remove (Customized)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\n\nstopword_set = set(nltk.corpus.stopwords.words('english'))  # English stopwords set for efficiency\nprint('nltk stopwords words \\n', stopword_set)\nprint(\" \")\n\n#  Keeping some necessary words\nwords_to_keep = {'don', 'wouldn', \"wasn't\", 'mustn', 'against', \"mightn't\", \"aren't\", 'won', 'aren', \"didn't\", 'should', 'haven', \n                 \"don't\", 'off', \"won't\", 'hadn', 'didn', 'shan', 'below', 'doesn', 'most', \"shan't\", 'than', 'mightn', \"isn't\",\n                 'needn', 'nor',  \"wouldn't\", 'not', 'above', \"needn't\", 'under', 'no', 'couldn', 'down', \"couldn't\", \"should've\",\n                 \"mustn't\", 'wasn', 'shouldn', 'but', 'ain', \"hadn't\", 'out', \"weren't\", \"doesn't\", 'over', 'hasn','weren', 'same', \n                 \"shouldn't\", \"haven't\", 'isn', \"hasn't\"}\n\n# Remove the selected words from the stopword set\nstopword_set -= words_to_keep\nprint('After keeping some necessary words nltk stopwords words \\n', stopword_set)\n\n# Define the remove_stopwords function\ndef custom_remove_stopwords(text, stopword_set):\n    tokens = word_tokenize(text)\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_set]\n    return ' '.join(filtered_tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatize","metadata":{}},{"cell_type":"code","source":"import spacy\nimport pandas as pd\n\n# Load the spaCy model\n# nlp = spacy.load('en_core_web_sm')\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# Define the lemmatize_texts function\ndef lemmatize_texts(texts, nlp):\n    docs = nlp.pipe(texts)    #     docs = nlp(texts)\n    return [' '.join([token.lemma_ for token in doc]) for doc in docs]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply preprocessing  to IMDb dataset","metadata":{}},{"cell_type":"code","source":"# Apply to IMDb dataset\n\n# Apply Data_cleaning_protocol\nimdb_dataset['review_P'] = imdb_dataset['review'].apply(Data_cleaning_protocol)\n# Apply Data_cleaning_protocol + custom_remove_stopwords\nimdb_dataset['review_PS'] = imdb_dataset['review_P'].apply(lambda x: custom_remove_stopwords(x, stopword_set))\n# Apply Data_cleaning_protocol + custom_remove_stopwords + lemmatization\nimdb_dataset['review_PSL'] = lemmatize_texts(imdb_dataset['review_PS'], nlp)\n# Apply Data_cleaning_protocol +lemmatization\nimdb_dataset['review_PL'] = lemmatize_texts(imdb_dataset['review_P'], nlp)\n\n## Save processed Data if you want\nimdb_dataset.to_csv(\"imdb_dataset_processed.csv\", index=False)\n# imdb_dataset.to_csv(\"PS(remove_stopwords)_imdb_dataset.csv\", index=False)\n# imdb_dataset.to_csv(\"PSL(lemmatize_texts_imdb)_dataset.csv\", index=False)\nprint('------------->imdb_dataset: I AM DONE<-------------------')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply preprocessing to USAirline dataset","metadata":{}},{"cell_type":"code","source":"# Apply to USAirline dataset\n\n# Apply Data_cleaning_protocol\nUSAirline_dataset['review_P'] = USAirline_dataset['review'].apply(Data_cleaning_protocol)\n# Apply Data_cleaning_protocol + custom_remove_stopwords\nUSAirline_dataset['review_PS'] = USAirline_dataset['review_P'].apply(lambda x: custom_remove_stopwords(x, stopword_set))\n# Apply Data_cleaning_protocol + custom_remove_stopwords + lemmatization\nUSAirline_dataset['review_PSL'] = lemmatize_texts(USAirline_dataset['review_PS'], nlp)\n# Apply Data_cleaning_protocol +lemmatization\nUSAirline_dataset['review_PL'] = lemmatize_texts(USAirline_dataset['review_P'], nlp)\n\n## Save processed Data if you want\nUSAirline_dataset.to_csv(\"USAirline_dataset_processed.csv\", index=False)\n# USAirline_dataset.to_csv(\"PS(remove_stopwords)_USAirline_dataset.csv\", index=False)\n# USAirline_dataset.to_csv(\"PSL(lemmatize_texts_imdb)_USAirline_dataset.csv\", index=False)\nprint('------------->USAirline_dataset: I AM DONE<-------------------')","metadata":{},"execution_count":null,"outputs":[]}]}