{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# USAirline","metadata":{}},{"cell_type":"markdown","source":"**LOAD + Handling Imbalanced Datasets + training-validation of proposed model**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom transformers import AutoTokenizer, TFAutoModel\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load dataset\ndataset = pd.read_csv(\"/kaggle/input/sentiments-processed/USAirline_dataset_processed.csv\")\ndataset = dataset.dropna()\n\n# Split dataset\nX = dataset['review_P']\ny = dataset['sentiment']\n\n# Prepare labels for oversampling check\ny_dummies = pd.get_dummies(y)\nclass_counts = y_dummies.sum()\nmax_class_count = class_counts.max()\n\n# Oversampling minority classes\nfor class_name, class_count in class_counts.iteritems():\n    samples_to_add = max_class_count - class_count\n    samples = dataset[dataset['sentiment'] == class_name].sample(n=samples_to_add, replace=True)\n    dataset = pd.concat([dataset, samples])\n\n# Ensure the dataset is still shuffled after oversampling\ndataset = dataset.sample(frac=1).reset_index(drop=True)\n\n# Now split your dataset\nX_train, X_temp, y_train, y_temp = train_test_split(dataset['review_P'], dataset['sentiment'], test_size=0.3, random_state=0)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n\n# Prepare labels again after balancing\ny_train = pd.get_dummies(y_train).values\ny_valid = pd.get_dummies(y_val).values\ny_test = pd.get_dummies(y_test).values\n\n# Rest of your code remains the same\n# Configuration\ntransformer_model = 'distilroberta-base'\ntokenizer = AutoTokenizer.from_pretrained(transformer_model)\nseq_len = 512\nbatch_size = 16\n\n# Tokenization\ndef tokenize_texts(texts):\n    return tokenizer(texts.tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n\ntokenized_inputs_train = tokenize_texts(X_train)\ntokenized_inputs_valid = tokenize_texts(X_val)\ntokenized_inputs_test = tokenize_texts(X_test)\n####################################################################################################################\n\n# Assuming transformer_model and seq_len have been defined\nencoder = TFAutoModel.from_pretrained(transformer_model)\ninput_ids = Input(shape=(seq_len,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(seq_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = encoder(input_ids, attention_mask=attention_mask)[0]\n####################################################################################################################\n#Adding Layers with distilroberta model \n# Correctly using layer variables\nbi_lstm_1 = Bidirectional(LSTM(256, return_sequences=True))(embeddings)\ndropout_1 = Dropout(0.1)(bi_lstm_1)\nlayer_norm_1 = LayerNormalization()(dropout_1)\nbi_lstm_2 = Bidirectional(LSTM(128, return_sequences=True))(layer_norm_1)\ndropout_2 = Dropout(0.2)(bi_lstm_2)\npooled_output = GlobalAveragePooling1D()(dropout_2)\ndense_1 = Dense(256, activation=\"relu\")(pooled_output)\ndropout_3 = Dropout(0.4)(dense_1)\noutputs = Dense(y_train.shape[1], activation='softmax')(dropout_3)  # Connecting the output through the intended final dropout layer\n\nmodel = Model(inputs=[input_ids, attention_mask], outputs=outputs)\nmodel.compile(optimizer=Adam(learning_rate=1e-5), loss=CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\nmodel.summary()\n####################################################################################################################\n# Prepare TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_train['input_ids'], 'attention_mask': tokenized_inputs_train['attention_mask']}, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_valid['input_ids'], 'attention_mask': tokenized_inputs_valid['attention_mask']}, y_valid))\n\n# EarlyStopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=4,\n    verbose=1,\n    restore_best_weights=True\n)\n\n# Training with EarlyStopping\nhistory = model.fit(\n    train_dataset.shuffle(10000).batch(batch_size),\n    validation_data=valid_dataset.batch(batch_size),\n    epochs=30,\n    verbose=1,\n    callbacks=[early_stopping]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate performance using Testing dataset**","metadata":{}},{"cell_type":"code","source":"# Prepare the test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_test['input_ids'], \n                                                    'attention_mask': tokenized_inputs_test['attention_mask']}, y_test))\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(test_dataset.batch(batch_size), verbose=1)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy and Loss Curve**","metadata":{}},{"cell_type":"markdown","source":"data generated from model.fit","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# New manually extracted accuracy and loss data during training time\n#data generated from model.fit\ntrain_accuracy = [0.8945, 0.9584, 0.9761, 0.9828, 0.9871, 0.9894, 0.9904, 0.9913, 0.9923, 0.9926, 0.9934, 0.9942]\nval_accuracy = [0.9523, 0.9659, 0.9751, 0.9816, 0.9833, 0.9831, 0.9850, 0.9852, 0.9845, 0.9840, 0.9847, 0.9833]\ntrain_loss = [0.2690, 0.1210, 0.0756, 0.0565, 0.0439, 0.0349, 0.0305, 0.0270, 0.0224, 0.0222, 0.0203, 0.0171]\nval_loss = [0.1351, 0.1077, 0.0856, 0.0664, 0.0656, 0.0668, 0.0638, 0.0610, 0.0662, 0.0703, 0.0677, 0.0797]\n\n# Identify the epochs of best accuracy and lowest loss\nbest_acc_epoch = val_accuracy.index(max(val_accuracy))  # 0-based indexing\nlowest_loss_epoch = val_loss.index(min(val_loss))  # 0-based indexing\n\n# Create figure for plotting\nplt.figure(figsize=(14, 6))\n\n# Plot training & validation accuracy with highlights\nplt.subplot(1, 2, 1)\nplt.plot(train_accuracy, 'bo--', label='Training Accuracy')\nplt.plot(val_accuracy, 'ro--', label='Validation Accuracy')\nplt.plot(best_acc_epoch, max(val_accuracy), 'ks', markersize=10, label='Best Validation Accuracy')\nplt.text(best_acc_epoch, max(val_accuracy), f'  Epoch {best_acc_epoch+1}\\n  {max(val_accuracy):.4f}', verticalalignment='bottom')\n#plt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\n\n# Plot training & validation loss with highlights\nplt.subplot(1, 2, 2)\nplt.plot(train_loss, 'bo--', label='Training Loss')\nplt.plot(val_loss, 'ro--', label='Validation Loss')\nplt.plot(lowest_loss_epoch, min(val_loss), 'ks', markersize=10, label='Lowest Validation Loss')\nplt.text(lowest_loss_epoch, min(val_loss), f'  Epoch {lowest_loss_epoch+1}\\n  {min(val_loss):.4f}', verticalalignment='top')\n#plt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\n\n# Adjust layout and save the figure\nplt.tight_layout()\nplt.savefig('/kaggle/working/Final_USAirline_curve_98.5.png', dpi=100)\n\n# Show the plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\npreds = model.predict(test_dataset.batch(batch_size))\ny_pred = np.argmax(preds, axis=1)\ny_true = np.argmax(y_test, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**confusion matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/Final_USAirline_Confusion_Matrix.png', dpi=100)\n\n# Show the plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification report**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Calculate and print the classification report\nprint(classification_report(y_true, y_pred, digits = 4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compute ROC curve and ROC area for each class**","metadata":{}},{"cell_type":"code","source":"# from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# First, ensure y_true is binarized for multiclass labels\nn_classes = len(np.unique(y_true))\ny_true_binarized = label_binarize(y_true, classes=np.arange(n_classes))\n\n# Compute ROC curve and ROC area for each class\nplt.figure()\nlw = 2\n\nfor i in range(n_classes):\n    fpr, tpr, _ = roc_curve(y_true_binarized[:, i], preds[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc))\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Multi-Class')\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.savefig('/kaggle/working/USAirline_ROC_CURVE.png', dpi=100)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMDB","metadata":{}},{"cell_type":"markdown","source":"**Load + training-validation of proposed model**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom transformers import AutoTokenizer, TFAutoModel\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load dataset\ndataset = pd.read_csv(\"/kaggle/input/sentiments-processed/imdb_dataset_processed.csv\")\ndataset = dataset.dropna()\n\n# Now split your dataset\nX_train, X_temp, y_train, y_temp = train_test_split(dataset['review_P'], dataset['sentiment'], test_size=0.3, random_state=0)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Prepare labels again after balancing\ny_train = pd.get_dummies(y_train).values\ny_valid = pd.get_dummies(y_val).values\ny_test = pd.get_dummies(y_test).values\n\n# Rest of your code remains the same\n# Configuration\ntransformer_model = 'distilroberta-base'\ntokenizer = AutoTokenizer.from_pretrained(transformer_model)\nseq_len = 512\nbatch_size = 16\n\n# Tokenization\ndef tokenize_texts(texts):\n    return tokenizer(texts.tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n\ntokenized_inputs_train = tokenize_texts(X_train)\ntokenized_inputs_valid = tokenize_texts(X_val)\ntokenized_inputs_test = tokenize_texts(X_test)\n\n####################################################################################################################\n\n# Assuming transformer_model and seq_len have been defined\nencoder = TFAutoModel.from_pretrained(transformer_model)\ninput_ids = Input(shape=(seq_len,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(seq_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = encoder(input_ids, attention_mask=attention_mask)[0]\n\n####################################################################################################################\n\n#Adding Layers with distilroberta model \n# Correctly using layer variables\nbi_lstm_1 = Bidirectional(LSTM(256, return_sequences=True))(embeddings)\ndropout_1 = Dropout(0.1)(bi_lstm_1)\nlayer_norm_1 = LayerNormalization()(dropout_1)\nbi_lstm_2 = Bidirectional(LSTM(128, return_sequences=True))(layer_norm_1)\ndropout_2 = Dropout(0.2)(bi_lstm_2)\npooled_output = GlobalAveragePooling1D()(dropout_2)\ndense_1 = Dense(256, activation=\"relu\")(pooled_output)\ndropout_3 = Dropout(0.4)(dense_1)\noutputs = Dense(y_train.shape[1], activation='softmax')(dropout_3)  # Connecting the output through the intended final dropout layer\n\nmodel = Model(inputs=[input_ids, attention_mask], outputs=outputs)\nmodel.compile(optimizer=Adam(learning_rate=1e-5), loss=CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\nmodel.summary()\n####################################################################################################################\n# Prepare TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_train['input_ids'], 'attention_mask': tokenized_inputs_train['attention_mask']}, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_valid['input_ids'], 'attention_mask': tokenized_inputs_valid['attention_mask']}, y_valid))\n\n# EarlyStopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=4,\n    verbose=1,\n    restore_best_weights=True\n)\n\n# Training with EarlyStopping\nhistory = model.fit(\n    train_dataset.shuffle(10000).batch(batch_size),\n    validation_data=valid_dataset.batch(batch_size),\n    epochs=30,\n    verbose=1,\n    callbacks=[early_stopping]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate performance using Testing dataset**","metadata":{}},{"cell_type":"code","source":"# Prepare the test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_test['input_ids'], \n                                                    'attention_mask': tokenized_inputs_test['attention_mask']}, y_test))\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(test_dataset.batch(batch_size), verbose=1)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy and Loss Curve**","metadata":{}},{"cell_type":"markdown","source":"data generated from model.fit","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# New manually extracted accuracy and loss data during training time\n#data generated from model.fit\ntrain_accuracy_updated = [0.9044, 0.9416, 0.9577, 0.9717, 0.9802, 0.9850, 0.9891]\nval_accuracy_updated = [0.9336, 0.9355, 0.9416, 0.9353, 0.9376, 0.9363, 0.9364]\ntrain_loss_updated = [0.2363, 0.1584, 0.1187, 0.0837, 0.0600, 0.0460, 0.0348]\nval_loss_updated = [0.1779, 0.1709, 0.1796, 0.2104, 0.2209, 0.2361, 0.2665]\n\n# Identify the epochs of best accuracy and lowest loss\nbest_acc_epoch_updated = val_accuracy_updated.index(max(val_accuracy_updated))  # 0-based indexing\nlowest_loss_epoch_updated = val_loss_updated.index(min(val_loss_updated))  # 0-based indexing\n\n# Create figure for plotting updated values\nplt.figure(figsize=(14, 6))\n\n# Plot updated training & validation accuracy with highlights\nplt.subplot(1, 2, 1)\nplt.plot(train_accuracy_updated, 'bo--', label='Training Accuracy')\nplt.plot(val_accuracy_updated, 'ro--', label='Validation Accuracy')\nplt.plot(best_acc_epoch_updated, max(val_accuracy_updated), 'ks', markersize=10, label='Best Validation Accuracy')\nplt.text(best_acc_epoch_updated, max(val_accuracy_updated), f'  Epoch {best_acc_epoch_updated+1}\\n  {max(val_accuracy_updated):.4f}', verticalalignment='bottom')\n#plt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\n\n# Plot updated training & validation loss with highlights\nplt.subplot(1, 2, 2)\nplt.plot(train_loss_updated, 'bo--', label='Training Loss')\nplt.plot(val_loss_updated, 'ro--', label='Validation Loss')\nplt.plot(lowest_loss_epoch_updated, min(val_loss_updated), 'ks', markersize=10, label='Lowest Validation Loss')\nplt.text(lowest_loss_epoch_updated, min(val_loss_updated), f'  Epoch {lowest_loss_epoch_updated+1}\\n  {min(val_loss_updated):.4f}', verticalalignment='top')\n#plt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\n\n# Adjust layout and save the figure\nplt.tight_layout()\nplt.savefig('/kaggle/working/imdb_acc_loss_curve_final.png', dpi=100)\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\npreds = model.predict(test_dataset.batch(batch_size))\ny_pred = np.argmax(preds, axis=1)\ny_true = np.argmax(y_test, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/imdb_Confusion_Matrix.png', dpi=100)\n\n# Show the plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Calculate and print the classification report\nprint(classification_report(y_true, y_pred, digits = 4))","metadata":{},"execution_count":null,"outputs":[]}]}